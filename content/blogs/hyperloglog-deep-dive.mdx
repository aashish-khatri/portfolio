---
title: 'HyperLogLog: A Deep Dive into Probabilistic Cardinality'
date: '2025-12-23'
description: 'How to count billions of unique elements with only 1.5KB of memory. We explore HyperLogLog, from its statistical origins to a production-ready Go implementation.'
tags: ['System Design', 'Algorithms', 'Go', 'Big Data']
---

In the world of big data, counting is deceptively simple. Counting *distinct* elements (cardinality) in a massive stream, however, is a fundamental engineering challenge. How do we count unique visitors to Google or unique IP addresses hitting a firewall without storing every single one?

> "The problem with exact counting is that it requires memory proportional to the number of unique elements. Probabilistic counting changes the game."

This article explores **HyperLogLog (HLL)**, the algorithm that powers Redis, BigQuery, and Elasticsearch cardinality aggregations.

## The Problem with Exact Sets

The naive approach to counting unique elements is using a Hash Set:

$$
\text{memory} \approx N \times \text{size}(\text{element})
$$

Where $N$ is the number of unique elements. If you are tracking IPv6 addresses (128-bit) and you have 1 billion unique users, the math gets ugly:

$$
10^9 \times 16 \text{ bytes} \approx 16 \text{ GB RAM}
$$

This makes real-time analytics on high-traffic streams impossible on standard hardware. We need a way to trade a small amount of accuracy for a massive reduction in memory.

## Enter HyperLogLog

HyperLogLog solves this by observing the "unlikeliness" of hashed values. It relies on a property of uniform hashing:

1.  Hash an element to a binary string.
2.  Count the number of leading zeros.
3.  The probability of seeing $k$ leading zeros is $0.5^k$.

If we see a hash with **10 leading zeros**, it is highly probable that we have seen approximately $2^{10}$ unique items to find that "rare" event.

### The Algorithm

Here is a robust implementation of HyperLogLog in Go. We use `math/bits` for efficient bit manipulation and bucket the hashes to reduce variance ($O(1)$ memory).

```go
package hyperloglog

import (
    "hash/fnv"
    "math"
    "math/bits"
)

const (
    // p = 14, m = 2^14 = 16384 buckets
    // This is the standard configuration for 0.81% error
    p = 14
    m = 1 << p
)

type HLL struct {
    registers []uint8 // The buckets
}

func New() *HLL {
    return &HLL{
        registers: make([]uint8, m),
    }
}

// Add inserts a value into the sketch
func (h *HLL) Add(data []byte) {
    hash := fnv32(data)
    
    // Use first p bits for bucket index
    j := hash >> (32 - p)
    
    // Use remaining bits to find leading zeros
    w := hash << p
    zeros := uint8(bits.LeadingZeros32(w)) + 1
    
    if zeros > h.registers[j] {
        h.registers[j] = zeros
    }
}

// fnv32 is a helper for non-cryptographic hashing
func fnv32(b []byte) uint32 {
    h := fnv.New32()
    h.Write(b)
    return h.Sum32()
}
```

### Harmonic Mean & Correction

The earlier **LogLog** algorithm used a simple geometric mean of the buckets, but it was sensitive to outliers (a single lucky hash with many zeros could skew the count).

HyperLogLog improves this by using the **Harmonic Mean** of the bucket values. This dampens the effect of outliers.

If $m$ is the number of registers (buckets) and $R_j$ is the max leading zeros in bucket $j$, the cardinality estimate $E$ is:

$$
E = \alpha_m \cdot m^2 \cdot \left( \sum_{j=1}^{m} 2^{-R_j} \right)^{-1}
$$

The standard error of this estimate is tightly bound:

$$
\sigma \approx \frac{1.04}{\sqrt{m}}
$$

For $m = 16,384$ (using just 16KB of memory), the error is only $\approx 0.81\%$.

## Complexity Analysis

Let's break down the performance characteristics:

| Operation | Complexity | Description |
| :--- | :--- | :--- |
| **Add** | $O(1)$ | Hash + Bit shift + Array write |
| **Merge** | $O(m)$ | Taking the max of registers from two HLLs |
| **Space** | $O(\log \log N)$ | Constant bucket count, tiny registers |

Where $N$ is the total cardinality of the stream. In practice, the space is often fixed (e.g., 12KB in Redis) regardless of $N$.

## Conclusion

HyperLogLog is a fundamental building block of modern data infrastructure. By using probabilistic data structures, we can solve "Big Data" problems on "Small Data" hardware.

Whether you are designing a real-time analytics dashboard, a distinct query optimizer, or a DDoS detection system, understanding this algorithm is essential for any backend engineer.
